# âš¡ SmartDoc RAG

**SmartDoc RAG** is an intelligent, AI-powered document assistant that enables users to interact with their documents using natural language. Built using **Streamlit**, **LangChain**, **ChromaDB**, and **Groq (Llama 3.1)**, it leverages Retrieval-Augmented Generation (RAG) to deliver accurate, context-aware answers with verified source citations.

It functions as a research assistant, study companion, and document analysis tool in a clean, modern UI.

---

## ğŸš€ Features

### ğŸ“‚ Multi-Document Upload
- Upload multiple PDF, TXT, or DOCX files.
- Automatically combines and indexes documents.
- Persistent vector storage using hash-based indexing.

### ğŸ§  Advanced RAG Pipeline
- Embedding-based retrieval using `sentence-transformers`.
- ChromaDB vector storage for semantic search.
- Similarity-based ranking for accurate context selection.

### ğŸ’¬ Context-Aware Conversational Chat
- Maintains conversation history.
- Supports natural follow-up queries.
- Handles greetings and recap questions intelligently.

### ğŸ’¡ Smart Follow-up Suggestions
- Automatically generates contextual follow-up questions.
- Clean 3-column suggestion layout.
- One-click interaction for seamless exploration.

### ğŸ“š Verified Source Attribution
- Displays the most relevant document source.
- Shows filename and page number.
- Minimal, research-style citation format.

### ğŸ› ï¸ AI Study Tools
From the sidebar, users can:
- ğŸ“„ Generate Document Summary
- ğŸ“ Generate Study Notes
- ğŸ¯ Generate Quiz Questions
- ğŸ“Œ Extract Key Topics
- ğŸ“¥ Export Chat to PDF

### âš¡ Streaming Responses
- Real-time token streaming for dynamic responses.
- Low latency inference using Groqâ€™s LPU engine.

### ğŸ¨ Modern UI
- Dark-themed, responsive interface.
- Clean chat-style layout.
- Minimal and distraction-free design.

---

## ğŸ› ï¸ Tech Stack

- **Frontend:** Streamlit  
- **LLM Framework:** LangChain (Modular Packages)  
- **Model:** Llama-3.1-8b-instant (via Groq)  
- **Embeddings:** sentence-transformers (all-MiniLM-L6-v2)  
- **Vector Database:** ChromaDB  
- **PDF Processing:** PyPDF  
- **Document Parsing:** docx2txt  
- **Environment Management:** python-dotenv  
- **Language:** Python 3.10+  

---

## ğŸ“‚ Project Structure

```bash
AI-Based-Document-Search/
â”œâ”€â”€ app.py            # Main Streamlit application
â”œâ”€â”€ rag_pipeline.py   # RAG logic & AI tools
â”œâ”€â”€ ingestion.py      # Document loading logic
â”œâ”€â”€ indexing.py       # Vectorstore creation (ChromaDB)
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ pdf_export.py   # Chat export functionality
â”œâ”€â”€ requirements.txt  # Project dependencies
â”œâ”€â”€ .env              # Environment variables (API Keys)
â”œâ”€â”€ data/             # Uploaded document storage
â””â”€â”€ chroma_db/        # Persistent vector database
```

## âš™ï¸ Installation & Setup

### 1. Clone the Repository
```bash
git clone [https://github.com/AdityaGupta171205/AI-Based-Document-Search.git](https://github.com/AdityaGupta171205/AI-Based-Document-Search.git)
cd AI-Based-Document-Search
```

### 2. Create a Virtual Environment
It is recommended to use a virtual environment to manage dependencies.

**Windows:**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**Mac/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```
*If `requirements.txt` is missing, install the core packages manually:*
```bash
pip install streamlit langchain-core langchain-community langchain-text-splitters langchain-groq langchain-huggingface chromadb sentence-transformers pypdf docx2txt python-dotenv reportlab
```

### 4. Configure Environment Variables
Create a `.env` file in the root directory and add your Groq API key:
```env
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸƒâ€â™‚ï¸ Usage

1ï¸âƒ£ Run the application:

2ï¸âƒ£ Open browser (default: `http://localhost:8501`)

3ï¸âƒ£ Upload one or more documents via the sidebar.

4ï¸âƒ£ Start chatting!

Example queries:
- â€œSummarize this document.â€
- â€œWhat are the key topics discussed?â€
- â€œGenerate quiz questions from this document.â€
- â€œWhat was my last question?â€
- â€œExplain this in simple terms.â€

---

## ğŸ§  How It Works (High-Level Architecture)

1. Documents are uploaded and parsed.
2. Text is split into chunks using recursive splitting.
3. Chunks are embedded using sentence-transformers.
4. Embeddings are stored in ChromaDB.
5. User query triggers similarity search.
6. Top relevant chunk is passed to Llama 3.1 via Groq.
7. Response is streamed with source citation.
8. Follow-up suggestions are generated dynamically.